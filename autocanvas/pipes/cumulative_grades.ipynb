{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi import Canvas\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from autocanvas.config import INPUT_DIR, OUTPUT_DIR\n",
    "\n",
    "from autocanvas.core.conversions import (\n",
    "    series_from_api_object, \n",
    "    df_from_api_list)\n",
    "\n",
    "from autocanvas.core.course_info import (\n",
    "    get_PHY_course, \n",
    "    get_assignment_group_from_name, \n",
    "    get_teaching_personel,\n",
    "    get_students_from_sections,)\n",
    "\n",
    "from autocanvas.core.assignments import (\n",
    "    get_assignment,\n",
    "    get_assignment_submissions,\n",
    "    get_graded_submissions,\n",
    "    get_submitted_submissions,\n",
    "    get_assignment_groups,\n",
    "    get_assignment_collection,\n",
    "    get_submissions_in_collection,)\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "API_URL = \"https://ufl.instructure.com/\"\n",
    "try:\n",
    "    from autocanvas.config import get_API_key\n",
    "    API_KEY = get_API_key()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    API_KEY = input(\"Asking for API token\")\n",
    "\n",
    "canvas = Canvas(API_URL, API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = get_PHY_course(canvas, \n",
    "                        course_code=\"PHY2054\", \n",
    "                        semester=\"Spring 2021\")\n",
    "print(course.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TAs, df_teachers = get_teaching_personel(\n",
    "                                course, \n",
    "                                add_first_name=True, \n",
    "                                groups=[\"ta\", \"teacher\"])\n",
    "\n",
    "file_name = \"section_ta_phy2054_spring2021.csv\"\n",
    "\n",
    "file_path = join(INPUT_DIR, file_name)\n",
    "df_students, df_sections = get_students_from_sections(\n",
    "                                course, \n",
    "                                section_ta_csv=file_path)\n",
    "print(\"Completed importing students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Grand Averages per TA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following page 11 of https://www.nist.gov/system/files/documents/2017/05/09/combine-1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quiz_number = 9\n",
    "df_regular_quizzes = get_assignment_collection(course,\n",
    "                             assignment_group_name=\"recitation quizzes\",\n",
    "                             name_pattern=\"^Quiz \\d+\",\n",
    "                             exclude_numbers=[0,10,13],\n",
    "                             add_identifier_numbers=True\n",
    "                            )\n",
    "collection = df_regular_quizzes[\n",
    "    df_regular_quizzes[\"identifier_number\"]<=max_quiz_number\n",
    "]\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_submissions = get_submissions_in_collection(\n",
    "                    assignment_collection=collection,\n",
    "                    df_students=df_students, \n",
    "                    df_TAs=df_TAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_makeup_quizzes = get_assignment_collection(course,\n",
    "                             assignment_group_name=\"recitation quizzes\",\n",
    "                             name_pattern=\"^Makeup (Q|q)uiz \\d+\",\n",
    "                             exclude_numbers=[0,10,13],\n",
    "                             add_identifier_numbers=True\n",
    "                            )\n",
    "makeup_collection = df_makeup_quizzes[\n",
    "    df_makeup_quizzes[\"identifier_number\"]<=max_quiz_number\n",
    "]\n",
    "all_makeup_submissions = \\\n",
    "    get_submissions_in_collection(\n",
    "                    assignment_collection=makeup_collection,\n",
    "                    df_students=df_students, \n",
    "                    df_TAs=df_TAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = [\"user_id\", \"name\", \"section_ta_first_name\"]\n",
    "use_makeups = True\n",
    "df_full = pd.DataFrame()\n",
    "for row_index, assignment in all_submissions.iterrows():\n",
    "    prefix = assignment[\"name\"]\n",
    "    print(prefix)\n",
    "    quiz_number = assignment[\"identifier_number\"]\n",
    "    \n",
    "    df_subs = assignment[\"submissions\"]\n",
    "    df_graded = get_graded_submissions(df_subs)\n",
    "    if use_makeups:\n",
    "        df_makeup_subs = all_makeup_submissions[\n",
    "            all_makeup_submissions[\"identifier_number\"]==quiz_number\n",
    "        ][\"submissions\"].iloc[0]\n",
    "        df_makeup_graded = get_graded_submissions(df_makeup_subs)\n",
    "        df_graded_combined = pd.concat((df_graded, df_makeup_graded))\n",
    "    else:\n",
    "        df_graded_combined = df_graded\n",
    "    \n",
    "    df_graded_combined = df_graded_combined.drop_duplicates(\n",
    "                                            subset=[\"user_id\"],\n",
    "                                            keep=\"last\")\n",
    "    \n",
    "    df_graded_combined = (df_graded_combined[common_columns+[\"grade\",]]\n",
    "                                .rename(columns={\"grade\": prefix+\" grade\"})\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not df_full.empty:\n",
    "        df_full = pd.merge(left=df_full, \n",
    "                           right=df_graded_combined, \n",
    "                           how=\"outer\", \n",
    "                           on=common_columns,\n",
    "                           validate=\"1:1\"\n",
    "                          )\n",
    "    else:\n",
    "        df_full = df_graded_combined\n",
    "\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_order = (df_full.section_ta_first_name\n",
    "            .drop_duplicates()\n",
    "            .sort_values()\n",
    "            .to_list()\n",
    "           )\n",
    "ta_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_columns = df_full.filter(regex='Quiz (\\d)+ grade').columns\n",
    "df_full[quiz_columns] = df_full[quiz_columns].astype(\"float\")\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_noncum_avg_columns = df_full.filter(regex='Quiz (\\d)+ grade').columns\n",
    "ta_quiz_avgs = (df_full.groupby(\"section_ta_first_name\")\n",
    "                        [quiz_noncum_avg_columns]\n",
    "                         .agg(\"mean\")\n",
    "               )\n",
    "print(ta_quiz_avgs.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quiz_idx, quiz_col in enumerate(quiz_columns):\n",
    "    \n",
    "    quiz_number = int(\"\".join(filter(str.isdigit, quiz_col)))\n",
    "    col_avg_name = \"quiz_average_upto_{}\".format(quiz_number)\n",
    "    df_full[col_avg_name] = (df_full[quiz_columns[:quiz_idx+1]]\n",
    "                             .mean(axis=1,skipna=True))\n",
    "df_full\n",
    "# df_full.groupby(\"section_ta_first\")[\"quiz_averages\"].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate of the standard error of the mean using student average quiz scores is:\n",
    "\n",
    "$$\\hat{u} = \\sqrt{\\frac{\\sum_{i=1}^k (\\bar{x}_i-\\bar{\\bar{x}})^2}{k(k-1)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci_mean(data, confidence_level=95):\n",
    "    \"\"\"\n",
    "    data: pd.Series, contains the measurements\n",
    "    confidence level: float or int, percentage from \n",
    "        0 to 100. It gives the confidence that the true \n",
    "        population mean lies within the confidence interval\n",
    "    \n",
    "    Returns tuple of confidence interval bounds\n",
    "    \"\"\"\n",
    "    from scipy.stats import t\n",
    "    n_sample = len(data)\n",
    "    degrees_of_freedom = n_sample - 1\n",
    "    \n",
    "    point_estimate_of_mean = data.mean()\n",
    "    sample_std_estimate = data.std()\n",
    "    standard_error_of_mean = (sample_std_estimate /\n",
    "                              np.sqrt(n_sample))\n",
    "    \n",
    "    cl = confidence_level / 100\n",
    "    \n",
    "    # scipy.stats.t.ppf gives the Inverse of the CDF \n",
    "    # so it is onesided\n",
    "    cl_one_sided = cl + (1-cl)/2\n",
    "    t_star = t.ppf(cl_one_sided, df=degrees_of_freedom)\n",
    "    \n",
    "    \n",
    "    lcb = point_estimate_of_mean - t_star * standard_error_of_mean\n",
    "    ucb = point_estimate_of_mean + t_star * standard_error_of_mean\n",
    "    \n",
    "    return lcb, ucb, standard_error_of_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def std_err_of_mean_of_means(data):\n",
    "    \"\"\"data is a series\"\"\"\n",
    "    # drop nan elements\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    lcb, ucb, std_err = get_ci_mean(data, confidence_level=95)\n",
    "    \n",
    "    return std_err\n",
    "\n",
    "\n",
    "def get_lower_bound_95(data):\n",
    "    \"\"\"data is a series\"\"\"\n",
    "    # drop nan elements\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    lcb, ucb, std_err = get_ci_mean(data, confidence_level=95)\n",
    "    \n",
    "    return lcb\n",
    "\n",
    "\n",
    "def get_upper_bound_95(data):\n",
    "    \"\"\"data is a series\"\"\"\n",
    "    # drop nan elements\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    lcb, ucb, std_err = get_ci_mean(data, confidence_level=95)\n",
    "    \n",
    "    return ucb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_avg_columns = df_full.filter(regex='quiz_average_upto_(\\d)+').columns\n",
    "means = (df_full.groupby(\"section_ta_first_name\")[quiz_avg_columns]\n",
    "         .agg(\"mean\")\n",
    "         .add_suffix(\"_mean\"))\n",
    "stds = (df_full.groupby(\"section_ta_first_name\")[quiz_avg_columns]\n",
    "        .agg(std_err_of_mean_of_means)\n",
    "        .add_suffix(\"_std\"))\n",
    "\n",
    "lcbs = (df_full.groupby(\"section_ta_first_name\")[quiz_avg_columns]\n",
    "        .agg(get_lower_bound_95)\n",
    "        .add_suffix(\"_lcb\"))\n",
    "\n",
    "ucbs = (df_full.groupby(\"section_ta_first_name\")[quiz_avg_columns]\n",
    "        .agg(get_upper_bound_95)\n",
    "        .add_suffix(\"_ucb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying what seaborn calculates in the following plot:\n",
    "summary_cumul_ta = pd.concat([means,stds,lcbs, ucbs], axis=1)\n",
    "print(summary_cumul_ta.filter(regex=\".*_mean$\").round(2).to_markdown())\n",
    "print(summary_cumul_ta.filter(regex=\".*_std$\").round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.wide_to_long(df_full, stubnames=\"quiz_average_upto_\", \n",
    "                j=\"Quiz number\", \n",
    "                i=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_long.reset_index(level=-1).copy()\n",
    "df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = sns.catplot(data=df_plot,\n",
    "            kind=\"point\",\n",
    "            dodge=0.3,\n",
    "#             capsize=.2,\n",
    "            height=10,\n",
    "            aspect=1.2,\n",
    "            ci=95,\n",
    "            join=False,\n",
    "             x=\"Quiz number\", \n",
    "             y=\"quiz_average_upto_\", \n",
    "             hue=\"section_ta_first_name\",\n",
    "            hue_order=ta_order\n",
    "           )\n",
    "plt.ylabel(\"Cumulative Average Grade\")\n",
    "plt.title(\"Progression of Quiz Averages per TA (Cumulative)\")\n",
    "bars.legend.set_title('')\n",
    "# plt.ylim(5.5,9)\n",
    "plt.gcf().subplots_adjust(bottom=0.15,left=0.15, top=0.9)\n",
    "plot_path = join(OUTPUT_DIR, \"grade_progression_TA.png\")\n",
    "plt.savefig(plot_path, \n",
    "            facecolor='w', \n",
    "            transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocanvas",
   "language": "python",
   "name": "autocanvas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
