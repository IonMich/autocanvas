{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi import Canvas # pip install canvasapi\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from autocanvas.config import INPUT_DIR, OUTPUT_DIR\n",
    "\n",
    "from autocanvas.core.conversions import (\n",
    "    series_from_api_object, \n",
    "    df_from_api_list)\n",
    "\n",
    "from autocanvas.core.course_info import (\n",
    "    get_PHY_course, \n",
    "    get_assignment_group_from_name, \n",
    "    get_teaching_personel,\n",
    "    get_students_from_sections,)\n",
    "\n",
    "from autocanvas.core.assignments import (\n",
    "    get_assignment,\n",
    "    get_assignment_submissions,\n",
    "    get_graded_submissions,\n",
    "    get_submitted_submissions,\n",
    "    get_assignment_groups,\n",
    "    get_assignment_collection,\n",
    "    get_submissions_in_collection,\n",
    "    get_student_answers,\n",
    "    get_quiz,\n",
    "    get_question_ids)\n",
    "\n",
    "# async routines are needed here, because there is no \n",
    "# API method to retrieve the questions for all students\n",
    "# so we need to to hundreds of calls.\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "API_URL = \"https://ufl.instructure.com/\"\n",
    "try:\n",
    "    from autocanvas.config import get_API_key\n",
    "    API_KEY = get_API_key()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    API_KEY = input(\"Asking for API token\")\n",
    "\n",
    "canvas = Canvas(API_URL, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get General course Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = get_PHY_course(canvas, \n",
    "                        course_code=\"PHY2054\", \n",
    "                        semester=\"Spring 2021\")\n",
    "print(course.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TAs, df_teachers = get_teaching_personel(course, add_first_name=True, \n",
    "                          groups=[\"ta\", \"teacher\"])\n",
    "\n",
    "file_name = \"section_ta_phy2054_spring2021.csv\"\n",
    "file_path = join(INPUT_DIR, file_name)\n",
    "\n",
    "df_students, df_sections = get_students_from_sections(\n",
    "                                course, \n",
    "                                section_ta_csv=file_path)\n",
    "print(\"Completed importing students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Assignment for Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_number = 9\n",
    "assignment = get_assignment(course, name=\"Quiz \"+str(quiz_number))\n",
    "assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs = get_assignment_submissions(assignment, \n",
    "                               df_students=df_students, \n",
    "                               df_TAs=df_TAs, \n",
    "                               include_submission_history=True)\n",
    "df_submitted = get_submitted_submissions(df_subs)\n",
    "print(len(df_submitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following field contains the submitted answers of a student\n",
    "df_submitted.iloc[19].submission_history[-1][\"submission_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_answers = get_student_answers(df_submitted)   \n",
    "df_student_answers.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corresponding Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Canvas API, has two methods of accessing a quiz asssignment. Each exposes different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = get_quiz(course, title=\"Quiz \"+str(quiz_number))\n",
    "quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_questions = list(quiz[\"object\"].iloc[0].get_questions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_from_api_list(quiz_questions, \n",
    "                                  drop_created_at=False, \n",
    "                                  bring_to_front=None)\n",
    "df_calculated_questions = df_questions[\n",
    "            df_questions[\"question_type\"]\n",
    "                         ==\"calculated_question\"].copy()\n",
    "df_calculated_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calculated_questions.iloc[0].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text of questions of interest\n",
    "text1 = ('If the radius of the loop is [r] cm, and it experiences a torque'\n",
    "        'of 0.174 N*m, what must be the strength of the magnetic field, in T?')\n",
    "\n",
    "text2 = (\"A circular loop of current, [I] A is placed in a [B] T magnetic field.\"\n",
    "         \" The field points straight up, while the loop is oriented so that a line \"\n",
    "         \"perpendicular to its plane makes an angle of 10 degrees with the magnetic \"\n",
    "         \"field. If the torque on the loop is 0.546 N*m, what is the radius of the loop, in cm?\")\n",
    "\n",
    "texts_to_match = [text1, text2]\n",
    "\n",
    "question_ids, closest_matches = get_question_ids(texts_to_match, \n",
    "                                                 df_calculated_questions, \n",
    "                                                 cutoff=0.3)\n",
    "\n",
    "df_bad_questions = df_questions[df_questions.index.isin(question_ids)]\n",
    "print(question_ids[0])\n",
    "print(closest_matches[0])\n",
    "print(question_ids[1])\n",
    "print(closest_matches[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_answers = df_student_answers[(df_student_answers\n",
    "                                         .question_id\n",
    "                                         .isin(question_ids))]\n",
    "\n",
    "# df_student_answers[\"question_version\"] = df_student_answers.apply(lambda x: 1 if x[\"question_id\"]==question_id1 else 2, axis=1)\n",
    "print(len(df_student_answers))\n",
    "\n",
    "print(df_student_answers.question_id\n",
    "                        .value_counts())\n",
    "\n",
    "df_student_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_submissions = list(quiz[\"object\"].iloc[0].get_submissions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Test student? is there one for quiz objects? \n",
    "# I have checked that it is not the last one\n",
    "# df_quiz_subs = df_quiz_subs[:-1]\n",
    "df_quiz_subs = df_from_api_list(quiz_submissions, \n",
    "                                drop_created_at=False, \n",
    "                                bring_to_front=None, \n",
    "                                set_index_id=False)\n",
    "# from now on we want to make sure we don't lose any submissions\n",
    "len_quiz_subs = len(df_quiz_subs)\n",
    "df_quiz_subs = pd.merge(left=df_quiz_subs, \n",
    "                        right=df_students, \n",
    "                   left_on=\"user_id\", right_on=\"id\",\n",
    "                    suffixes=(\"\",\"_student\"),\n",
    "                   how=\"left\",validate=\"m:1\")\n",
    "df_quiz_subs = pd.merge(left=df_quiz_subs, \n",
    "                   right=(df_TAs.reset_index()[[\"id\",\n",
    "                                                \"name\",\n",
    "                                                \"object\",\n",
    "                                                \"first_name\"]]\n",
    "                                .add_prefix(\"section_ta_\")\n",
    "                         ), \n",
    "                   on=\"section_ta_name\",\n",
    "                   how=\"left\", validate=\"m:1\")\n",
    "assert len(df_quiz_subs)==len_quiz_subs, \"Data were lost unexpectedly\"\n",
    "print(len(df_quiz_subs))  \n",
    "df_quiz_subs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_submissions[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# result = requests.get(API_URL+\"api/v1/\"+\"quiz_submissions/{}/questions/\".format(21067791),\n",
    "#       headers={'Authorization': 'Bearer {}'.format(access_token)})\n",
    "\n",
    "# result.json().get(\"quiz_submission_questions\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gather_questions():\n",
    "    async def get_sub_questions(submission_id):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(API_URL+\"api/v1/\"+\"quiz_submissions/{}/questions/\".format(submission_id),\n",
    "                                      headers={'Authorization': 'Bearer {}'.format(API_KEY)}) as resp:\n",
    "                if resp.status==200:\n",
    "                    all_sub_questions[submission_id] = await resp.json()\n",
    "                else:\n",
    "                    await resp.text()\n",
    "                    print(resp.status)\n",
    "    # need to deal with high watermark:\n",
    "    # https://community.canvaslms.com/t5/Developers-Group/API-Rate-Limiting/ba-p/255845\n",
    "    # suggestion of groups of 30 every 0.250 seconds:\n",
    "    # https://community.canvaslms.com/t5/Developers-Group/API-Rate-Limiting/m-p/211140\n",
    "    rate_limit = 30\n",
    "    all_sub_questions = {}\n",
    "    len_q_subs = len(quiz_submissions)\n",
    "    bunchstart = 0\n",
    "    while bunchstart < len_q_subs:\n",
    "        print(bunchstart)\n",
    "        coros = [get_sub_questions(submission_id) for submission_id in df_quiz_subs.id[bunchstart:bunchstart+rate_limit]]\n",
    "        await asyncio.gather(*coros)\n",
    "        await asyncio.sleep(0.250)\n",
    "        bunchstart += rate_limit \n",
    "        \n",
    "    return all_sub_questions\n",
    "\n",
    "all_sub_questions = await gather_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(quiz_submissions))\n",
    "print(len(all_sub_questions))\n",
    "assert len(quiz_submissions)==len(all_sub_questions), \"Data were lost. Decrease request rate.\"\n",
    "all_sub_questions[df_quiz_subs.id.iloc[7]]['quiz_submission_questions'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_inputs_list = []\n",
    "for submission_id, sub_qs in all_sub_questions.items():\n",
    "    for question in sub_qs[\"quiz_submission_questions\"]:\n",
    "        if question[\"id\"] in question_ids:\n",
    "            variables_dict = {}\n",
    "            \n",
    "#             print(submission_id)\n",
    "#             print(question[\"answers\"][0])\n",
    "            variables = question[\"answers\"][0][\"variables\"]\n",
    "            question_name = question['question_name']\n",
    "            position = question['position']\n",
    "#             print(variables)\n",
    "            for variable in variables:\n",
    "                variables_dict[variable[\"name\"]] = float(variable[\"value\"])\n",
    "            variables_dict[\"input\"] = variables_dict.copy()\n",
    "            variables_dict[\"question_name\"] = question_name\n",
    "            variables_dict[\"position\"] = position\n",
    "            variables_dict[\"submission_id\"] = submission_id\n",
    "            question_inputs_list.append(variables_dict)\n",
    "df_inputs = pd.DataFrame(question_inputs_list) \n",
    "df_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge on submission_id with quiz_subs to add student_name and section_TA_first \n",
    "- Merge on student name with answers df.\n",
    "- Write function that calculates the correct answer\n",
    "- compare with text (i.e. student_answer) to get a revised correct indicator.\n",
    "- compare with current correct indicator to decide whether they need regrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs = pd.merge(left=df_inputs, \n",
    "                       right=(df_quiz_subs[[\"id\", \n",
    "                                            \"name\", \n",
    "                                            \"quiz_version\",\n",
    "                                            \"sortable_name\", \n",
    "                                            \"class_number\",\n",
    "                                            \"section_ta_first_name\"]]\n",
    "                              .rename(columns={\"id\":\"submission_id\"})), \n",
    "                       on=\"submission_id\", \n",
    "                       how=\"left\", \n",
    "                       validate=\"1:1\")\n",
    "df_inputs[\"input\"] = df_inputs.apply(lambda x: {**x[\"input\"],\n",
    "                                                **{\"quiz_version\":\n",
    "                                                   int(x[\"quiz_version\"])\n",
    "                                                  }\n",
    "                                               }, \n",
    "                                     axis=1)\n",
    "\n",
    "df_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(left=df_student_answers, \n",
    "                       right=(df_inputs\n",
    "                              .rename(columns={\"name\":\"student_name\"})\n",
    "                             ), \n",
    "                       on=\"student_name\", \n",
    "                       how=\"right\", \n",
    "                       validate=\"1:1\")\n",
    "df_combined[\"input\"] = df_combined.apply(lambda x: {**x[\"input\"],\n",
    "                                                    **{\"question_id\":\n",
    "                                                       x[\"question_id\"],\n",
    "                                                       \"position\":\n",
    "                                                       x[\"position\"]},\n",
    "                                                   }, axis=1)\n",
    "\n",
    "df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"input\"].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate new answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quiz 9 PHY2054 Spring 2021 Q1 both versions on Tuesday\n",
    "def get_correct_B_field(I, r, quiz_version=None, \n",
    "                        decimals=4, **_):\n",
    "    #Q1V1\n",
    "    if quiz_version==25.0:\n",
    "        answer = 1/(3.14*I*r*r/10000) * 0.985/0.174\n",
    "    elif quiz_version==26.0:\n",
    "        answer = 1/(3.14*I*r*r/10000)\n",
    "    else:\n",
    "        raise ValueError(\"Quiz version is invalid\")\n",
    "    return round(answer,decimals)\n",
    "\n",
    "def get_correct_radius(I, B, quiz_version=None, decimals=2, **_):\n",
    "    #Q1V2\n",
    "    if quiz_version==25.0:\n",
    "        answer = 100/np.sqrt(I*B) * np.sqrt(0.985/0.174)\n",
    "    elif quiz_version==26.0:\n",
    "        answer = 100/np.sqrt(I*B)\n",
    "    \n",
    "    return round(answer,decimals)\n",
    "\n",
    "def get_correct_answer(input_dict):\n",
    "    \"\"\"\n",
    "    Use indices of `texts_to_match` entries to get the \n",
    "    corresponding index of `question_ids`\n",
    "    \n",
    "    \"\"\"\n",
    "    if (input_dict[\"position\"]==1 and \n",
    "        input_dict[\"question_id\"]==question_ids[0]):\n",
    "        return get_correct_B_field(**input_dict)\n",
    "    elif (input_dict[\"position\"]==1 and \n",
    "        input_dict[\"question_id\"]==question_ids[1]):\n",
    "        return get_correct_radius(**input_dict)\n",
    "    else:\n",
    "        raise ValueError(\"Question version is invalid\")\n",
    "    \n",
    "    \n",
    "\n",
    "example_vars = {\"I\":30.3, \"r\":18.62,\n",
    "                \"position\":1,\n",
    "                \"quiz_version\":25.0, \n",
    "                \"question_id\":question_ids[0]}\n",
    "get_correct_answer(example_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(student_answer, correct_answer, \n",
    "               answer_tolerance_percentage=\"2%\"):\n",
    "    # TODO: check for max truncation error of correct answer and raise warning if too big\n",
    "    tolerance = float(answer_tolerance_percentage.strip(\"%\"))/100\n",
    "    assert abs(correct_answer) > 1E-4, \"Correct answer is very close to zero, so I cannot calculate relative difference\"\n",
    "    rel_diff = (student_answer - correct_answer) / correct_answer\n",
    "    if abs(rel_diff) < tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_correct(student_answer=6, correct_answer=6.1, \n",
    "           answer_tolerance_percentage=\"2%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"correct_answer\"] = df_combined.apply(lambda x:get_correct_answer(x[\"input\"]),axis=1)\n",
    "df_combined[\"student_answer\"] = (df_combined[\"text\"]\n",
    "                                     .str.replace(',', '')\n",
    "                                     .replace(\"\",\"NaN\")\n",
    "                                     .astype(float)\n",
    "                                )\n",
    "df_combined[\"revised_correct\"] = df_combined.apply(lambda x: is_correct(x[\"student_answer\"], x[\"correct_answer\"]), axis=1)\n",
    "df_combined[\"needs_manual_review\"] = df_combined.apply(lambda x: np.logical_xor(x[\"revised_correct\"], x[\"correct\"]), axis=1)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review = df_combined[df_combined[\"needs_manual_review\"]]\n",
    "print(len(df_manual_review))\n",
    "df_manual_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[df_manual_review[\"section_ta_first_name\"]==\"Ivan\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_review_ta_counts = df_manual_review[\"section_ta_first_name\"].value_counts()\n",
    "manual_review_ta_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[[\"correct\",\"revised_correct\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[\"quiz_version\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change store to files to True in order to save the results of the analysis\n",
    "store_results = False\n",
    "for ta in manual_review_ta_counts.index:\n",
    "    df_to_save = (df_manual_review[df_manual_review[\"section_ta_first_name\"]==ta]\n",
    "                  [[\"sortable_name\",\n",
    "                    \"class_number\",\n",
    "                    \"quiz_version\",\n",
    "                    \"question_name\",\n",
    "                    \"position\",\n",
    "                    \"question_id\",\n",
    "                    \"correct_answer\",\n",
    "                    \"student_answer\",\n",
    "                    \"correct\",\n",
    "                    \"revised_correct\",\n",
    "                    \"needs_manual_review\"]]\n",
    "                  .rename(columns={\"correct\":\"old_correct\"})\n",
    "                  .sort_values(by=[\"class_number\",\"sortable_name\"])\n",
    "                 )\n",
    "    if store_results:\n",
    "        output_path = join(OUTPUT_DIR , \"{}_needs_manual_review_quiz_{}.csv\"\n",
    "                                           .format(ta.lower(), quiz_number)\n",
    "                          )\n",
    "        df_to_save.to_csv(output_path, \n",
    "                          index=False)\n",
    "df_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No safe way to do this, since TAs might have given fudge points insteach of regrading the numerical part. So skipping for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`QuizSubmission.update_score_and_comments`](https://canvasapi.readthedocs.io/en/stable/quiz-ref.html#canvasapi.quiz.QuizSubmission.update_score_and_comments)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycanvas",
   "language": "python",
   "name": "pycanvas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
