{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi import Canvas\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from autocanvas.config import INPUT_DIR, OUTPUT_DIR\n",
    "\n",
    "from autocanvas.core.conversions import (\n",
    "    series_from_api_object, \n",
    "    df_from_api_list)\n",
    "\n",
    "from autocanvas.core.course_info import (\n",
    "    get_PHY_course, \n",
    "    get_assignment_group_from_name, \n",
    "    get_teaching_personel,\n",
    "    get_students_from_sections,)\n",
    "\n",
    "from autocanvas.core.assignments import (\n",
    "    get_assignment,\n",
    "    get_assignment_submissions,\n",
    "    get_graded_submissions,\n",
    "    get_submitted_submissions,\n",
    "    get_assignment_groups,\n",
    "    get_assignment_collection,\n",
    "    get_submissions_in_collection,)\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "API_URL = \"https://ufl.instructure.com/\"\n",
    "try:\n",
    "    from autocanvas.config import get_API_key\n",
    "    API_KEY = get_API_key()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    API_KEY = input(\"Asking for API token\")\n",
    "\n",
    "canvas = Canvas(API_URL, API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = get_PHY_course(canvas, \n",
    "                        course_code=\"PHY2054\", \n",
    "                        semester=\"Spring 2021\")\n",
    "print(course.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TAs, df_teachers = get_teaching_personel(course, add_first_name=True, \n",
    "                          groups=[\"ta\", \"teacher\"])\n",
    "\n",
    "file_name = \"section_ta_phy2054_spring2021.csv\"\n",
    "file_path = join(INPUT_DIR, file_name)\n",
    "\n",
    "df_students, df_sections = get_students_from_sections(\n",
    "                                course, \n",
    "                                section_ta_csv=file_path)\n",
    "print(\"Completed importing students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_number = 9\n",
    "\n",
    "assignment = get_assignment(course, group=\"Recitation Quizzes\", \n",
    "               number=quiz_number, ignore_makeup=True)\n",
    "\n",
    "df_subs = get_assignment_submissions(\n",
    "                                assignment=assignment, \n",
    "                                df_students=df_students, \n",
    "                                df_TAs=df_TAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graded = get_graded_submissions(df_subs)\n",
    "df_submitted = get_submitted_submissions(df_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs.groupby(\"workflow_state\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_graded)\n",
    "len(df_submitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corresponding Makeup Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeup_name = \"Makeup Quiz {}\".format(quiz_number)\n",
    "makeup_assignment = get_assignment(course, name=makeup_name)\n",
    "print(makeup_assignment[\"name\"])\n",
    "df_makeup_subs = get_assignment_submissions(\n",
    "                                assignment=makeup_assignment, \n",
    "                                df_students=df_students, \n",
    "                                df_TAs=df_TAs)\n",
    "print(\"Number of makeups:\", len(df_makeup_subs))\n",
    "df_makeup_graded = get_graded_submissions(df_makeup_subs)\n",
    "print(\"Number of graded makeups:\", len(df_makeup_graded))\n",
    "df_makeup_submitted = get_submitted_submissions(df_makeup_subs)\n",
    "print(\"Number of submitted makeups:\", len(df_makeup_submitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options that the instructs have for the \"status\" of the student submission are: `excused`, `missing`, `late`, `None`. The internal status the Canvas keeps track can be: `graded`, `pending_review`, `submitted`, `unsubmitted`. Students with no submission normally appear with `missing==True`, `excused==None` and `status==unsubmitted`. If the TA puts a grade in these students and then removes it these students will appear **still** with `missing==True`, but with `excused==False` and `status==graded` (weirdly, despite that Canvas will have an orange circle, instead of a tickmark). If the TA excuses the student (with or without submission) then `excused==True`, `missing==False` and `status==graded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graded_combined = pd.concat((df_graded, df_makeup_graded))\n",
    "# student that appear with valid graded submissions multiple times\n",
    "df_graded_combined[df_graded_combined.user_id.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_graded))\n",
    "print(len(df_makeup_graded))\n",
    "print(len(df_graded_combined))\n",
    "print(len(df_graded_combined.user_id) - len(df_graded_combined.user_id.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates and keep last. \"Last\" is currently not very robust - make it submission date depenent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graded_combined = df_graded_combined.drop_duplicates(subset=[\"user_id\"],keep=\"last\")\n",
    "# should be zero\n",
    "print(len(df_graded_combined.user_id) - len(df_graded_combined.user_id.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TA Grades: Plots and Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_order = (df_subs.section_ta_first_name\n",
    "            .drop_duplicates()\n",
    "            .sort_values()\n",
    "            .to_list()\n",
    "           )\n",
    "ta_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make figure more wide to fit all points, otherwise points might be hidden\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.swarmplot(data=df_graded_combined, \n",
    "              y=\"score\", \n",
    "              x=\"grader_first_name\",\n",
    "              s=2,\n",
    "              order=ta_order)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Student scores\", fontsize=20)\n",
    "plt.axhline(y=7.5, c=\"k\", ls=\"--\", zorder=0)\n",
    "plt.ylim(0, 10.5)\n",
    "plt.title(assignment.name.iloc[0], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plot_path = join(OUTPUT_DIR, \"swarmplot_quiz_{}.png\"\n",
    "                             .format(quiz_number))\n",
    "plt.savefig(plot_path, \n",
    "            facecolor='w', \n",
    "            transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(data=df_graded_combined, \n",
    "            y=\"score\", \n",
    "            x=\"grader_first_name\",\n",
    "            order=ta_order\n",
    "           )\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Student scores\", fontsize=20)\n",
    "plt.axhline(y=7.5, c=\"k\", ls=\"--\", zorder=0)\n",
    "plt.ylim(0, 10.5)\n",
    "plt.title(assignment.name.iloc[0], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plot_path = join(OUTPUT_DIR, \"boxplot_quiz_{}.png\"\n",
    "                             .format(quiz_number))\n",
    "plt.savefig(plot_path, \n",
    "            facecolor='w', \n",
    "            transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.violinplot(data=df_graded_combined, \n",
    "               y=\"score\", \n",
    "               x=\"grader_first_name\",\n",
    "               order=ta_order\n",
    "              )\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Student scores\", fontsize=20)\n",
    "plt.axhline(y=7.5, c=\"k\", ls=\"--\", zorder=0)\n",
    "plt.ylim(0, 10.5)\n",
    "plt.title(assignment.name.iloc[0], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plot_path = join(OUTPUT_DIR,\"violinplot_quiz_{}.png\"\n",
    "                             .format(quiz_number))\n",
    "plt.savefig(plot_path, \n",
    "            facecolor='w', \n",
    "            transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_graded_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = df_graded_combined.groupby(\"grader_first_name\").mean()[\"score\"]\n",
    "print(averages.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = df_graded_combined.groupby(\"grader_first_name\").median()[\"score\"]\n",
    "print(medians.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Check Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submitted[df_submitted[\"workflow_state\"]!=\"graded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All due dates should be after submission dates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submitted[(df_submitted[\"cached_due_date_date\"] < df_submitted[\"submitted_at_date\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader TA should be the section TA. If it is not, then the grade might be incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graded[(df_graded[\"grader_first_name\"] != df_graded[\"section_ta_first_name\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naively I expected that all cached due dates would be strictly less than graded_at_dates.  \n",
    "That's not the case because:\n",
    "1. some TAs might grade while they wait for their recitation session to end.\n",
    "2. weirdly, when you award a fudge point, the actual graded_at_date gets overwritten\n",
    "   by the student submitted_at_date. The same might be happening with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the overwritten graded_at_dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwritten_graded_times = df_graded[df_graded[\"submitted_at_date\"] == df_graded[\"graded_at_date\"]]\n",
    "len(overwritten_graded_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But still none should be graded before it is submitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_than_light = df_graded[df_graded[\"submitted_at_date\"] > df_graded[\"graded_at_date\"]]\n",
    "len(faster_than_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Time Evolution by TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are used to check how the grading is progressing. Ignoring Makeups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_submitted_ta = (df_submitted.groupby(by=\"section_ta_first_name\")\n",
    "                                  .workflow_state.count()\n",
    "                                  .rename(\"total_submitted\")\n",
    "                     )\n",
    "total_submitted_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_ta = df_submitted.pivot_table(index=\"section_ta_first_name\", columns=\"workflow_state\", \n",
    "                    aggfunc=\"count\", values=\"user_id\").fillna(0)\n",
    "df_status_ta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get earliest possible date for each TA grading, \n",
    "# defined to be equal to the earliest student submission date for each TA\n",
    "# used to generate the starting point for progress\n",
    "from datetime import timedelta\n",
    "\n",
    "earliest_times = (df_submitted\n",
    "                  [[\"submitted_at_date\", \"section_ta_first_name\"]].copy()\n",
    "                    .dropna(axis=0)\n",
    "                    .groupby(by=\"section_ta_first_name\").agg(\"min\")\n",
    "                    .reset_index(drop=False)\n",
    "                    .rename(columns={\"submitted_at_date\": \"graded_at\"})\n",
    "                 )\n",
    "# nothing graded yet, that's why the grade counter is still zero\n",
    "earliest_times[\"occurences\"] = 0\n",
    "earliest_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graded_at = (df_graded[[\"graded_at_date\",\"section_ta_first_name\"]].copy()\n",
    "                        .rename(columns={\"graded_at_date\": \"graded_at\"}))\n",
    "df_graded_at[\"occurences\"] = 1\n",
    "df_graded_at = pd.concat([earliest_times, df_graded_at]).reset_index(drop=True)\n",
    "\n",
    "# group by TA and by seconds\n",
    "delta_t = '1s'\n",
    "df_graded_at = df_graded_at.groupby([pd.Grouper(key='section_ta_first_name'),\n",
    "                                     pd.Grouper(key='graded_at', freq=delta_t)\n",
    "                                    ]).sum()\n",
    "df_graded_at[df_graded_at['occurences']==0]\n",
    "# calculate cumulative sum for each TA\n",
    "df_graded_at['cumsum'] = df_graded_at.groupby(level=0)['occurences'].cumsum()\n",
    "\n",
    "df_graded_at.reset_index(inplace=True)\n",
    "\n",
    "df_graded_at[\"progress\"] = (df_graded_at.set_index('section_ta_first_name')[\"cumsum\"] / \n",
    "                            total_submitted_ta[df_graded_at[\"section_ta_first_name\"]]).reset_index(drop=True) * 100\n",
    "df_graded_at.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_graded_at[df_graded_at[\"section_ta_first\"]==\"Ioannis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there is a bug on the stored `graded_at` times and many times are overwritten by the `submitted_at` time, which is at the time of the quiz. The bug seems to affect TA comments and Fudge points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "up_to_now = False\n",
    "plt.figure(figsize=(15,10))\n",
    "marker = 'o' if delta_t=='1s' else None \n",
    "g = sns.lineplot(data=df_graded_at, x=\"graded_at\", y=\"progress\", \n",
    "             hue=\"section_ta_first_name\", drawstyle=\"steps-post\", \n",
    "                 marker=marker, legend=None);\n",
    "plt.xticks(rotation=30);\n",
    "plt.axhline(y=100,c=\"k\",zorder=0,lw=0.5,ls=\"--\")\n",
    "\n",
    "# deadline at end of Wednesday ET\n",
    "# 5 hour difference between ET and UTC\n",
    "grading_deadline = pd.to_datetime(earliest_times[\"graded_at\"].min().date()) \\\n",
    "                                  + pd.DateOffset(days=9, hours=5)\n",
    "print(grading_deadline)\n",
    "\n",
    "plt.axvline(x=grading_deadline,c=\"k\",zorder=0,lw=2.5,ls=\"--\", label=\"Deadline\")\n",
    "if up_to_now:\n",
    "    plt.axvline(x=datetime.utcnow(),c=\"m\",zorder=0,lw=2.5,ls=\"-.\", label=\"Present\")\n",
    "plt.ylim(0,100)\n",
    "plt.title(\"{} Grading Progress\".format(assignment.name.iloc[0]))\n",
    "\n",
    "plt.xlabel(\"Graded At (UTC)\");\n",
    "lines = g.get_lines()\n",
    "for index, ta_first in enumerate(ta_order):\n",
    "    line_name = \"Line2D(_line{})\".format(index)\n",
    "    line_2d = [x for x in lines if line_name==x.__str__()][0]\n",
    "#     print(ta_first, line_2d.__str__())\n",
    "    missing = int(total_submitted_ta.loc[ta_first] - \n",
    "                  df_status_ta.loc[ta_first].graded)\n",
    "#     print(missing)\n",
    "    line_2d.set_label(\"{} ({})\".format(ta_first, missing))\n",
    "plt.legend(loc='lower right')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.tight_layout()\n",
    "plot_path = join(OUTPUT_DIR, \"quiz_{}_grading_progress.png\"\n",
    "                             .format(quiz_number))\n",
    "plt.savefig(plot_path, \n",
    "            facecolor='w', \n",
    "            transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add manually points to the line2d artists\n",
    "# import numpy as np\n",
    "# artists = plt.gca().get_children()\n",
    "\n",
    "# for index, ta_first in enumerate(ta_order):\n",
    "#     line_name = \"Line2D(_line{})\".format(index)\n",
    "#     line_2d = [x for x in artists if line_name==x.__str__()][0]\n",
    "#     print(ta_first, line_2d.__str__())\n",
    "#     x_start = earliest_times.iloc[index]\n",
    "#     y_start = 0\n",
    "#     xdata = np.insert(line_2d.get_xdata(), obj=0, values=x_start)\n",
    "#     print(xdata)\n",
    "#     ydata = np.insert(line_2d.get_ydata(), obj=0, values=y_start)\n",
    "#     line_2d.set_data\n",
    "# item.set_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycanvas",
   "language": "python",
   "name": "pycanvas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
