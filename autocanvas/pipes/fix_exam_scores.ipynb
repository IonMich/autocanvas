{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi import Canvas # pip install canvasapi\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from autocanvas.config import INPUT_DIR, OUTPUT_DIR\n",
    "\n",
    "from autocanvas.core.conversions import (\n",
    "    series_from_api_object, \n",
    "    df_from_api_list)\n",
    "\n",
    "from autocanvas.core.course_info import (\n",
    "    get_PHY_course, \n",
    "    get_assignment_group_from_name, \n",
    "    get_teaching_personel,\n",
    "    get_students_from_sections,)\n",
    "\n",
    "from autocanvas.core.assignments import (\n",
    "    get_assignment,\n",
    "    get_assignment_submissions,\n",
    "    get_graded_submissions,\n",
    "    get_submitted_submissions,\n",
    "    get_assignment_groups,\n",
    "    get_assignment_collection,\n",
    "    get_submissions_in_collection,\n",
    "    get_student_answers,\n",
    "    get_quiz,\n",
    "    get_question_ids)\n",
    "\n",
    "# async routines are needed here, because there is no \n",
    "# API method to retrieve the questions for all students\n",
    "# so we need to to hundreds of calls.\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "API_URL = \"https://ufl.instructure.com/\"\n",
    "try:\n",
    "    from autocanvas.config import get_API_key\n",
    "    API_KEY = get_API_key()\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    API_KEY = input(\"Asking for API token\")\n",
    "\n",
    "canvas = Canvas(API_URL, API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get General course Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course = get_PHY_course(canvas, \n",
    "                        course_code=\"PHY2053\", \n",
    "                        semester=\"Summer C 2020\")\n",
    "print(course.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TAs, df_teachers = get_teaching_personel(course, add_first_name=True, \n",
    "                          groups=[\"ta\", \"teacher\"])\n",
    "\n",
    "file_name = \"sections_phy2053_summer2020.csv\"\n",
    "file_path = join(INPUT_DIR, file_name)\n",
    "df_students, df_sections = get_students_from_sections(\n",
    "                                course, \n",
    "                                section_ta_csv=file_path)\n",
    "print(\"Completed importing students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Assignment for Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_number = 1\n",
    "assignment = get_assignment(course, name=\"Exam \"+str(exam_number))\n",
    "assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subs = get_assignment_submissions(assignment, \n",
    "                               df_students=df_students, \n",
    "                               df_TAs=df_TAs, \n",
    "                               include_submission_history=True)\n",
    "df_submitted = get_submitted_submissions(df_subs)\n",
    "print(len(df_submitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following field contains the submitted answers of a student\n",
    "df_submitted.iloc[19].submission_history[-1][\"submission_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the past I have found students with multiple submissions\n",
    "assert \"submission_history\" in df_submitted.columns\n",
    "answers_list = []\n",
    "for index, submission in df_submitted.iterrows():\n",
    "    user_id = submission.user_id\n",
    "#     print(submission.sortable_name)\n",
    "    student_name = submission[\"name\"]\n",
    "    if len(submission.submission_history)!=1:\n",
    "        print(submission.sortable_name,\n",
    "              len(submission.submission_history))\n",
    "#         raise ValueError\n",
    "    assert submission.submission_history[-1][\"submission_data\"] is not None\n",
    "    sub_data = submission.submission_history[-1][\"submission_data\"]\n",
    "    for question_data in sub_data:\n",
    "        question_data[\"user_id\"] = user_id\n",
    "        question_data[\"student_name\"] = student_name\n",
    "        answers_list.append(question_data)\n",
    "    \n",
    "df_student_answers = pd.DataFrame(answers_list)\n",
    "df_student_answers.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_student_answers = get_student_answers(df_submitted)   \n",
    "# df_student_answers.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corresponding Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Canvas API, has two methods of accessing a quiz asssignment. Each exposes different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = get_quiz(course, title=\"Exam \"+str(exam_number))\n",
    "quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: this fails for concluded courses (access denied). Same is true for the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_questions = list(quiz[\"object\"].iloc[0].get_questions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = df_from_api_list(quiz_questions, \n",
    "                                  drop_created_at=False, \n",
    "                                  bring_to_front=None)\n",
    "df_multiple_choice_questions = df_questions[\n",
    "            df_questions[\"question_type\"]\n",
    "                         ==\"multiple_choice_question\"].copy()\n",
    "df_multiple_choice_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multiple_choice_questions.iloc[23].question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = df_multiple_choice_questions[\n",
    "    df_multiple_choice_questions.question_name\n",
    "    .str.contains('Q1[1|2].*')].index.tolist()\n",
    "\n",
    "question_ids\n",
    "df_bad_questions = df_questions[df_questions.index.isin(question_ids)]\n",
    "df_bad_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_student_answers = df_student_answers[(df_student_answers\n",
    "                                         .question_id\n",
    "                                         .isin(question_ids))]\n",
    "\n",
    "# df_student_answers[\"question_version\"] = df_student_answers.apply(lambda x: 1 if x[\"question_id\"]==question_id1 else 2, axis=1)\n",
    "print(len(df_student_answers))\n",
    "\n",
    "print(df_student_answers.question_id\n",
    "                        .value_counts())\n",
    "\n",
    "df_student_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers_in_bad_questions = df_student_answers[\n",
    "        df_student_answers\n",
    "        .question_id.isin(question_ids)][[\"student_name\", \"user_id\", \"correct\"]]\n",
    "\n",
    "df_scores_in_bad_questions = (df_answers_in_bad_questions.groupby(\n",
    "                                            [\"student_name\", \"user_id\"])\n",
    "                                .sum().reset_index().rename(columns={\"correct\":\n",
    "                                                             \"scores_in_bad\"})\n",
    "                             )\n",
    "df_scores_in_bad_questions[\"scores_in_bad\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_submissions = list(quiz[\"object\"].iloc[0].get_submissions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Test student? is there one for quiz objects? \n",
    "# I have checked that it is not the last one\n",
    "# df_quiz_subs = df_quiz_subs[:-1]\n",
    "df_quiz_subs = df_from_api_list(quiz_submissions, \n",
    "                                drop_created_at=False, \n",
    "                                bring_to_front=None, \n",
    "                                set_index_id=False)\n",
    "# from now on we want to make sure we keep all submissions\n",
    "len_quiz_subs = len(df_quiz_subs)\n",
    "df_quiz_subs = pd.merge(left=df_quiz_subs, \n",
    "                        right=df_students, \n",
    "                   left_on=\"user_id\", right_on=\"id\",\n",
    "                    suffixes=(\"\",\"_student\"),\n",
    "                   how=\"left\",validate=\"m:1\")\n",
    "df_quiz_subs = pd.merge(left=df_quiz_subs, \n",
    "                        right=df_scores_in_bad_questions[[\"user_id\",\"scores_in_bad\"]], \n",
    "                   left_on=\"user_id\", right_on=\"user_id\",\n",
    "                   how=\"left\",validate=\"1:1\")\n",
    "assert len(df_quiz_subs)==len_quiz_subs, \"Data were lost unexpectedly\"\n",
    "print(len(df_quiz_subs))  \n",
    "df_quiz_subs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quiz_subs.score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quiz_subs[\"updated_score\"] = df_quiz_subs.score - df_quiz_subs.scores_in_bad\n",
    "df_quiz_subs[\"updated_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_scores = df_quiz_subs[[\"sortable_name\", \"name\",\"score\", \"updated_score\"]]\n",
    "df_new_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = join(OUTPUT_DIR , \"phy2053_exam_1_new_scores.csv\"\n",
    "                          )\n",
    "\n",
    "df_new_scores.to_csv(output_path, \n",
    "                     index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# (Skip for multiple choice Exams) Exam Question Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_submissions[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# result = requests.get(API_URL+\"api/v1/\"+\"quiz_submissions/{}/questions/\".format(21067791),\n",
    "#       headers={'Authorization': 'Bearer {}'.format(access_token)})\n",
    "\n",
    "# result.json().get(\"quiz_submission_questions\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gather_questions():\n",
    "    async def get_sub_questions(submission_id):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(API_URL+\"api/v1/\"+\"quiz_submissions/{}/questions/\".format(submission_id),\n",
    "                                      headers={'Authorization': 'Bearer {}'.format(API_KEY)}) as resp:\n",
    "                if resp.status==200:\n",
    "                    all_sub_questions[submission_id] = await resp.json()\n",
    "                else:\n",
    "                    await resp.text()\n",
    "                    print(resp.status)\n",
    "    # need to deal with high watermark:\n",
    "    # https://community.canvaslms.com/t5/Developers-Group/API-Rate-Limiting/ba-p/255845\n",
    "    # suggestion of groups of 30 every 0.250 seconds:\n",
    "    # https://community.canvaslms.com/t5/Developers-Group/API-Rate-Limiting/m-p/211140\n",
    "    rate_limit = 30\n",
    "    all_sub_questions = {}\n",
    "    len_q_subs = len(quiz_submissions)\n",
    "    bunchstart = 0\n",
    "    while bunchstart < len_q_subs:\n",
    "        print(bunchstart)\n",
    "        coros = [get_sub_questions(submission_id) for submission_id in df_quiz_subs.id[bunchstart:bunchstart+rate_limit]]\n",
    "        await asyncio.gather(*coros)\n",
    "        await asyncio.sleep(0.250)\n",
    "        bunchstart += rate_limit \n",
    "        \n",
    "    return all_sub_questions\n",
    "\n",
    "all_sub_questions = await gather_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(quiz_submissions))\n",
    "print(len(all_sub_questions))\n",
    "assert len(quiz_submissions)==len(all_sub_questions), \"Data were lost. Decrease request rate.\"\n",
    "all_sub_questions[df_quiz_subs.id.iloc[7]]['quiz_submission_questions'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_inputs_list = []\n",
    "for submission_id, sub_qs in all_sub_questions.items():\n",
    "    for question in sub_qs[\"quiz_submission_questions\"]:\n",
    "        if question[\"id\"] in question_ids:\n",
    "            variables_dict = {}\n",
    "            \n",
    "#             print(submission_id)\n",
    "#             print(question[\"answers\"][0])\n",
    "            variables = question[\"answers\"][0][\"variables\"]\n",
    "            question_name = question['question_name']\n",
    "            position = question['position']\n",
    "#             print(variables)\n",
    "            for variable in variables:\n",
    "                variables_dict[variable[\"name\"]] = float(variable[\"value\"])\n",
    "            variables_dict[\"input\"] = variables_dict.copy()\n",
    "            variables_dict[\"question_name\"] = question_name\n",
    "            variables_dict[\"position\"] = position\n",
    "            variables_dict[\"submission_id\"] = submission_id\n",
    "            question_inputs_list.append(variables_dict)\n",
    "df_inputs = pd.DataFrame(question_inputs_list) \n",
    "df_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge on submission_id with quiz_subs to add student_name and section_TA_first \n",
    "- Merge on student name with answers df.\n",
    "- Write function that calculates the correct answer\n",
    "- compare with text (i.e. student_answer) to get a revised correct indicator.\n",
    "- compare with current correct indicator to decide whether they need regrading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputs = pd.merge(left=df_inputs, \n",
    "                       right=(df_quiz_subs[[\"id\", \n",
    "                                            \"name\", \n",
    "                                            \"quiz_version\",\n",
    "                                            \"sortable_name\", \n",
    "                                            \"class_number\",\n",
    "                                            \"section_ta_first_name\"]]\n",
    "                              .rename(columns={\"id\":\"submission_id\"})), \n",
    "                       on=\"submission_id\", \n",
    "                       how=\"left\", \n",
    "                       validate=\"1:1\")\n",
    "df_inputs[\"input\"] = df_inputs.apply(lambda x: {**x[\"input\"],\n",
    "                                                **{\"quiz_version\":\n",
    "                                                   int(x[\"quiz_version\"])\n",
    "                                                  }\n",
    "                                               }, \n",
    "                                     axis=1)\n",
    "\n",
    "df_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(left=df_student_answers, \n",
    "                       right=(df_inputs\n",
    "                              .rename(columns={\"name\":\"student_name\"})\n",
    "                             ), \n",
    "                       on=\"student_name\", \n",
    "                       how=\"right\", \n",
    "                       validate=\"1:1\")\n",
    "df_combined[\"input\"] = df_combined.apply(lambda x: {**x[\"input\"],\n",
    "                                                    **{\"question_id\":\n",
    "                                                       x[\"question_id\"],\n",
    "                                                       \"position\":\n",
    "                                                       x[\"position\"]},\n",
    "                                                   }, axis=1)\n",
    "\n",
    "df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"input\"].iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# (Skip for multiple choice Exams) Calculate new answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phy2053 summer 2021 Quiz 1 Question 3 Version 1 was missing 1.2\n",
    "def get_correct_time(d, t2, quiz_version=None, \n",
    "                        decimals=4, **_):\n",
    "    answer = (d - 1.2*t2-0.5*0.1*t2**2)/1.2\n",
    "    return round(answer,decimals)\n",
    "\n",
    "#quiz 9 PHY2054 Spring 2021 Q1 both versions on Tuesday\n",
    "def get_correct_B_field(I, r, quiz_version=None, \n",
    "                        decimals=4, **_):\n",
    "    #Q1V1\n",
    "    if quiz_version==25.0:\n",
    "        answer = 1/(3.14*I*r*r/10000) * 0.985/0.174\n",
    "    elif quiz_version==26.0:\n",
    "        answer = 1/(3.14*I*r*r/10000)\n",
    "    else:\n",
    "        raise ValueError(\"Quiz version is invalid\")\n",
    "    return round(answer,decimals)\n",
    "\n",
    "def get_correct_radius(I, B, quiz_version=None, decimals=2, **_):\n",
    "    #Q1V2\n",
    "    if quiz_version==25.0:\n",
    "        answer = 100/np.sqrt(I*B) * np.sqrt(0.985/0.174)\n",
    "    elif quiz_version==26.0:\n",
    "        answer = 100/np.sqrt(I*B)\n",
    "    \n",
    "    return round(answer,decimals)\n",
    "\n",
    "def get_correct_answer(input_dict):\n",
    "    \"\"\"\n",
    "    Use indices of `texts_to_match` entries to get the \n",
    "    corresponding index of `question_ids`\n",
    "    \n",
    "    \"\"\"\n",
    "    if (input_dict[\"position\"]==3 and \n",
    "        input_dict[\"question_id\"]==question_ids[0]):\n",
    "        return get_correct_time(**input_dict)\n",
    "#     elif (input_dict[\"position\"]==1 and \n",
    "#         input_dict[\"question_id\"]==question_ids[1]):\n",
    "#         return get_correct_radius(**input_dict)\n",
    "    else:\n",
    "        raise ValueError(\"Question version is invalid\")\n",
    "    \n",
    "print(question_ids[0])\n",
    "\n",
    "example_vars = {\"d\":66, \"t2\":19,\n",
    "                \"position\":3, \n",
    "                \"question_id\":question_ids[0]}\n",
    "print(example_vars[\"position\"])\n",
    "get_correct_answer(example_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(student_answer, correct_answer, \n",
    "               answer_tolerance_percentage=\"2%\"):\n",
    "    # TODO: check for max truncation error of correct answer and raise warning if too big\n",
    "    tolerance = float(answer_tolerance_percentage.strip(\"%\"))/100\n",
    "    assert abs(correct_answer) > 1E-4, \"Correct answer is very close to zero, so I cannot calculate relative difference\"\n",
    "    rel_diff = (student_answer - correct_answer) / correct_answer\n",
    "    if abs(rel_diff) < tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_correct(student_answer=6, correct_answer=6.1, \n",
    "           answer_tolerance_percentage=\"2%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"correct_answer\"] = df_combined.apply(lambda x:get_correct_answer(x[\"input\"]),axis=1)\n",
    "df_combined[\"student_answer\"] = (df_combined[\"text\"]\n",
    "                                     .str.replace(',', '')\n",
    "                                     .replace(\"\",\"NaN\")\n",
    "                                     .astype(float)\n",
    "                                )\n",
    "df_combined[\"revised_correct\"] = df_combined.apply(lambda x: is_correct(x[\"student_answer\"], x[\"correct_answer\"]), axis=1)\n",
    "df_combined[\"needs_manual_review\"] = df_combined.apply(lambda x: np.logical_xor(x[\"revised_correct\"], x[\"correct\"]), axis=1)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review = df_combined[df_combined[\"needs_manual_review\"]]\n",
    "print(len(df_manual_review))\n",
    "df_manual_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[df_manual_review[\"section_ta_first_name\"]==\"Ioannis\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_review_ta_counts = df_manual_review[\"section_ta_first_name\"].value_counts()\n",
    "manual_review_ta_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[[\"correct\",\"revised_correct\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual_review[\"quiz_version\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change store to files to True in order to save the results of the analysis\n",
    "store_results = True\n",
    "for ta in manual_review_ta_counts.index:\n",
    "    df_to_save = (df_manual_review[df_manual_review[\"section_ta_first_name\"]==ta]\n",
    "                  [[\"sortable_name\",\n",
    "                    \"class_number\",\n",
    "                    \"quiz_version\",\n",
    "                    \"question_name\",\n",
    "                    \"position\",\n",
    "                    \"question_id\",\n",
    "                    \"correct_answer\",\n",
    "                    \"student_answer\",\n",
    "                    \"correct\",\n",
    "                    \"revised_correct\",\n",
    "                    \"needs_manual_review\"]]\n",
    "                  .rename(columns={\"correct\":\"old_correct\"})\n",
    "                  .sort_values(by=[\"class_number\",\"sortable_name\"])\n",
    "                 )\n",
    "    if store_results:\n",
    "        output_path = join(OUTPUT_DIR , \"{}_needs_manual_review_quiz_{}.csv\"\n",
    "                                           .format(ta.lower(), quiz_number)\n",
    "                          )\n",
    "        df_to_save.to_csv(output_path, \n",
    "                          index=False)\n",
    "df_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipping for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [`QuizSubmission.update_score_and_comments`](https://canvasapi.readthedocs.io/en/stable/quiz-ref.html#canvasapi.quiz.QuizSubmission.update_score_and_comments)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycanvas",
   "language": "python",
   "name": "pycanvas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
